{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T00:50:09.744376Z",
     "start_time": "2020-08-13T00:50:09.571540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "import gc\n",
    "gc.enable()\n",
    "import re\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T00:50:13.525842Z",
     "start_time": "2020-08-13T00:50:10.786248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = sparknlp.start(gpu=False)\n",
    "sparknlp.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T00:50:14.490382Z",
     "start_time": "2020-08-13T00:50:14.488515Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Spark NLP \n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T00:52:48.184125Z",
     "start_time": "2020-08-13T00:52:48.134508Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf = spark.read.parquet('toxicity.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T00:52:49.820413Z",
     "start_time": "2020-08-13T00:52:49.816663Z"
    }
   },
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"comment_text\") \\\n",
    "    .setOutputCol(\"document\")\\\n",
    "    .setCleanupMode('shrink') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T00:52:53.236830Z",
     "start_time": "2020-08-13T00:52:53.137270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|                                                                                                                                                                       col|toxic|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|                                                                                                                              COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK|    1|\n",
      "|Hey... what is it.. @ | talk . What is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks th...|    1|\n",
      "|                                                                                                                   Bye! Don't look, come or think of comming back! Tosser.|    1|\n",
      "|You are gay or antisemmitian? Archangel WHite Tiger Meow! Greetingshhh! Uh, there are two ways, why you do erased my comment about WW2, that holocaust was brutally sla...|    1|\n",
      "|                                                                                                                                  FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!|    1|\n",
      "|I'm Sorry I'm sorry I screwed around with someones talk page. It was very bad to do. I know how having the templates on their talk page helps you assert your dominance...|    1|\n",
      "|                                                GET FUCKED UP. GET FUCKEEED UP. GOT A DRINK THAT YOU CANT PUT DOWN???/ GET FUCK UP GET FUCKED UP. I'M FUCKED UP RIGHT NOW!|    1|\n",
      "|                                                                                 Stupid peace of shit stop deleting my stuff asshole go die and fall in a hole go to hell!|    1|\n",
      "|                                                                                                      =Tony Sidaway is obviously a fistfuckee. He loves an arm up his ass.|    1|\n",
      "|My Band Page's deletion. You thought I was gone. Deleting the comment I posted on your 'talk page' does not delete my feelings for your hasty decision to delete my pag...|    1|\n",
      "|Why can't you believe how fat Artie is? Did you see him on his recent appearence on the Tonight Show with Jay Leno? He looks absolutely AWFUL! If I had to put money on...|    1|\n",
      "|All of my edits are good. Cunts like you who revert good edits because you're too stupid to understand how to write well , and then revert other edits just because you...|    1|\n",
      "|                                                                                                          Hi! I am back again! Last warning! Stop undoing my edits or die!|    1|\n",
      "|                                                                                                 Would you both shut up, you don't run wikipedia, especially a stupid kid.|    1|\n",
      "|                                                                                                                                A pair of jew-hating weiner nazi schmucks.|    1|\n",
      "|\" SORRY PUCK BUT NO ONE EVER SAID DICK WAS NUMBER ONE!! But he is clearly listed as being \"\"second only to Batman\"\" and a \"\"master detective and martial artist\"\" and b...|    1|\n",
      "|                                                                        \" UNBLOCK ME OR I'LL GET MY LAWYERS ON TO YOU FOR BLOCKING MY CONSTITUTIONAL RIGHT TO FREE SPEECH\"|    1|\n",
      "|                                     You should be fired, you're a moronic wimp who is too lazy to do research. It makes me sick that people like you exist in this world.|    1|\n",
      "|                                          I think that your a Fagget get a oife and burn in Hell I hate you 'm sorry we cant have any more sex i'm running out of conndoms|    1|\n",
      "|                                                                                                                       you are a stupid fuck and your mother's cunt stinks|    1|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_assembler.transform(sdf).select(F.explode(\"document.result\"),'toxic').show(truncate=170)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T00:54:48.444172Z",
     "start_time": "2020-08-13T00:54:48.439272Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_latest_log(path):\n",
    "   \n",
    "    from pathlib import Path\n",
    "    import pendulum\n",
    "    \n",
    "    files=[*Path(path).iterdir()]\n",
    "    files=[(file,file.stat().st_ctime) for file in files]\n",
    "    \n",
    "    a=sorted(files, key=lambda x:x[1],reverse=True)[0]\n",
    "    \n",
    "    with open(a[0],'r') as f:\n",
    "        b=f.readlines()\n",
    "    \n",
    "    print(pendulum.from_timestamp(a[1],tz=pendulum.local_timezone()).to_cookie_string(),\"\\n\")\n",
    "    \n",
    "    [print(b_) for b_ in b]\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T00:53:15.028973Z",
     "start_time": "2020-08-13T00:53:12.127285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"comment_text\") \\\n",
    "    .setOutputCol(\"document\")\\\n",
    "    .setCleanupMode('shrink') \n",
    "\n",
    "use = UniversalSentenceEncoder.pretrained()\\\n",
    " .setInputCols([\"document\"])\\\n",
    " .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "\n",
    "sentimentClassifier = SentimentDLApproach()\\\n",
    "      .setInputCols(\"sentence_embeddings\")\\\n",
    "      .setOutputCol(\"prediction\")\\\n",
    "      .setLabelColumn(\"toxic\")\\\n",
    "      .setBatchSize(128)\\\n",
    "      .setMaxEpochs(10)\\\n",
    "      .setDropout(0.7)\\\n",
    "      .setValidationSplit(0.2)\\\n",
    "\n",
    "\n",
    "clf_pipeline = Pipeline(\n",
    "    stages=[document_assembler, \n",
    "            use,\n",
    "            sentimentClassifier\n",
    "           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T00:53:54.157147Z",
     "start_time": "2020-08-13T00:53:16.318457Z"
    }
   },
   "outputs": [],
   "source": [
    "clf_pipelineModel = clf_pipeline.fit(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T00:54:53.073675Z",
     "start_time": "2020-08-13T00:54:53.013176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wednesday, 12-Aug-2020 12:58:14 CDT \n",
      "\n",
      "Training started - total epochs: 5 - learning rate: 0.0035 - batch size: 32 - training examples: 27530\n",
      "\n",
      "Epoch 0/5 - 3.724994516%.2fs - loss: 369.49588 - accuracy: 0.87215847 - validation: 89.27404 - batches: 861\n",
      "\n",
      "Epoch 1/5 - 3.115798814%.2fs - loss: 347.8156 - accuracy: 0.90617734 - validation: 90.255066 - batches: 861\n",
      "\n",
      "Epoch 2/5 - 3.110044603%.2fs - loss: 339.1991 - accuracy: 0.9194041 - validation: 89.96076 - batches: 861\n",
      "\n",
      "Epoch 3/5 - 3.107633562%.2fs - loss: 333.6996 - accuracy: 0.9277253 - validation: 90.05886 - batches: 861\n",
      "\n",
      "Epoch 4/5 - 3.106006482%.2fs - loss: 327.55127 - accuracy: 0.93277615 - validation: 90.15697 - batches: 861\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/aytu/annotator_logs\"\n",
    "print_latest_log(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T00:55:46.394717Z",
     "start_time": "2020-08-13T00:55:38.299491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n",
      "bert_base_uncased download started this may take some time.\n",
      "Approximate size to download 392.5 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"comment_text\") \\\n",
    "    .setOutputCol(\"document\")\\\n",
    "    .setCleanupMode('shrink') \n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"token\")\n",
    "    \n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\")\\\n",
    "    .setLowercase(True)\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "      .setInputCols(\"normalized\")\\\n",
    "      .setOutputCol(\"cleanTokens\")\\\n",
    "      .setCaseSensitive(False)\n",
    "\n",
    "lemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"lemma\")\n",
    "\n",
    "bert = BertEmbeddings.pretrained('bert_base_uncased', 'en') \\\n",
    "      .setInputCols(\"document\", \"lemma\") \\\n",
    "      .setOutputCol(\"embeddings\")\\\n",
    "      .setPoolingLayer(0) # default 0\n",
    "\n",
    "embeddingsSentence = SentenceEmbeddings() \\\n",
    "      .setInputCols([\"document\", \"embeddings\"]) \\\n",
    "      .setOutputCol(\"sentence_embeddings\") \\\n",
    "      .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "\n",
    "\n",
    "classsifierdl = ClassifierDLApproach()\\\n",
    "  .setInputCols([\"sentence_embeddings\"])\\\n",
    "  .setOutputCol(\"prediction\")\\\n",
    "  .setLabelColumn(\"toxic\")\\\n",
    "  .setMaxEpochs(5)\\\n",
    "  .setEnableOutputLogs(True)\\\n",
    "  .setBatchSize(32)\\\n",
    "  .setValidationSplit(0.1)\\\n",
    "  .setDropout(0.75)\\\n",
    "  .setLr(0.0035)\\\n",
    "  #.setOutputLogsPath('logs')\n",
    "\n",
    "clf_pipeline = Pipeline(\n",
    "    stages=[document_assembler, \n",
    "            tokenizer,\n",
    "            normalizer,\n",
    "            stopwords_cleaner, \n",
    "            lemma, \n",
    "            bert,\n",
    "            embeddingsSentence,\n",
    "            classsifierdl\n",
    "           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T00:58:56.353360Z",
     "start_time": "2020-08-13T00:55:47.594454Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_pipelineModel = clf_pipeline.fit(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T00:59:04.352667Z",
     "start_time": "2020-08-13T00:59:04.349654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wednesday, 12-Aug-2020 19:58:56 CDT \n",
      "\n",
      "Training started - total epochs: 5 - learning rate: 0.0035 - batch size: 32 - training examples: 27530\n",
      "\n",
      "Epoch 0/5 - 3.765714683%.2fs - loss: 372.9806 - accuracy: 0.87136626 - validation: 89.24133 - batches: 861\n",
      "\n",
      "Epoch 1/5 - 3.104467391%.2fs - loss: 342.52994 - accuracy: 0.9053052 - validation: 89.37214 - batches: 861\n",
      "\n",
      "Epoch 2/5 - 3.106660562%.2fs - loss: 335.56335 - accuracy: 0.9181686 - validation: 89.30673 - batches: 861\n",
      "\n",
      "Epoch 3/5 - 3.164794216%.2fs - loss: 331.0061 - accuracy: 0.92659885 - validation: 89.27404 - batches: 861\n",
      "\n",
      "Epoch 4/5 - 3.222160764%.2fs - loss: 325.64188 - accuracy: 0.9313227 - validation: 89.30673 - batches: 861\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_latest_log(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
